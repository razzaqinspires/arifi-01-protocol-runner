#!/usr/bin/env python3
"""
arifi_protocol_runner.py
Implementasi lokal (CLI) dari Protokol Fondasi: Matrik Keterikatan Arifi-01
Fungsi:
 - Menerima permintaan (file JSON atau teks) dari "Entitas_Pemandu" (alias: Adik)
 - Menghasilkan file kode lengkap berdasarkan template / konten input
 - Menjalankan analisis statis/minimal & metrik (jika alat tersedia)
 - Membuat perbandingan sederhana terhadap versi sebelumnya (jika ada)
 - Mengeluarkan rekomendasi peningkatan
 - Menyajikan ringkasan dan meminta konfirmasi lanjutan (console)
Catatan: Semua langkah dijalankan lokal. Skrip TIDAK mengunggah data ke layanan eksternal.
"""

import argparse
import json
import os
import shutil
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Dict, Optional, Tuple

# ---------------------------
# Util helpers
# ---------------------------
def now_ts() -> str:
    return datetime.utcnow().isoformat() + "Z"

def read_text_file(path: Path) -> str:
    return path.read_text(encoding="utf-8")

def write_text_file(path: Path, content: str) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(content, encoding="utf-8")

def run_cmd(cmd: list, timeout: int = 10) -> Tuple[int, str, str]:
    try:
        p = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)
        return p.returncode, p.stdout.strip(), p.stderr.strip()
    except Exception as e:
        return 2, "", f"Exception: {e}"

# ---------------------------
# Core: generator & analyzer
# ---------------------------

DEFAULT_PY_TEMPLATE = '''"""
Generated by Arifi-01 protocol runner
Generated at: {ts}
Source: {source_label}
"""

from __future__ import annotations

def generated_entry():
    """
    Entry point function â€” replace with full implementation requested by 'Adik'.
    This file is intentionally self-contained and executable.
    """
    # TODO: implement actual logic as requested
    return "Hello from generated code"

if __name__ == "__main__":
    print(generated_entry())
'''

def generate_code_from_prompt(prompt_text: str, language: str, out_path: Path, metadata: Dict) -> None:
    """
    Very simple generator: for Python, produce a full .py module from template and
    include the prompt as a comment. For JS/others, produce a placeholder file.
    (Real generation can replace this function.)
    """
    ts = now_ts()
    if language.lower().startswith("py"):
        content = f"# Prompt:\n# {prompt_text.replace(chr(10),'\\n# ')}\n\n" + DEFAULT_PY_TEMPLATE.format(ts=ts, source_label=metadata.get("source","Adik"))
        write_text_file(out_path, content)
    elif language.lower().startswith("js"):
        content = f"// Prompt:\n// {prompt_text.replace(chr(10),'\\n// ')}\n\nconsole.log('Placeholder generated JS file');\n"
        write_text_file(out_path, content)
    else:
        content = f"/* Prompt:\n{prompt_text}\n*/\n\n# Unsupported language placeholder\n"
        write_text_file(out_path, content)

def compute_basic_metrics(file_path: Path) -> Dict:
    txt = read_text_file(file_path)
    lines = txt.count("\n") + 1
    chars = len(txt)
    words = len(txt.split())
    # approximate functions by keyword occurrences (very rough)
    func_count = txt.count("def ") + txt.count("function ") + txt.count("=>")
    return {
        "path": str(file_path),
        "lines": lines,
        "chars": chars,
        "words": words,
        "func_like_count": func_count
    }

def run_static_checks(file_path: Path) -> Dict:
    """
    Attempt to run tools if installed: flake8, mypy, radon (for Python).
    If missing, report as not-available.
    """
    results = {}
    # flake8
    rc, out, err = run_cmd(["flake8", str(file_path)], timeout=20)
    results["flake8"] = {"rc": rc, "stdout": out, "stderr": err}

    # mypy (type checking)
    rc, out, err = run_cmd(["mypy", str(file_path)], timeout=20)
    results["mypy"] = {"rc": rc, "stdout": out, "stderr": err}

    # radon cc (cyclomatic complexity)
    rc, out, err = run_cmd(["radon", "cc", "-s", str(file_path)], timeout=20)
    results["radon_cc"] = {"rc": rc, "stdout": out, "stderr": err}

    return results

def load_previous_metrics(out_file: Path) -> Optional[Dict]:
    meta_file = out_file.with_suffix(out_file.suffix + ".meta.json")
    if meta_file.exists():
        try:
            return json.loads(meta_file.read_text(encoding="utf-8"))
        except Exception:
            return None
    return None

def save_metrics(out_file: Path, metrics: Dict) -> None:
    meta_file = out_file.with_suffix(out_file.suffix + ".meta.json")
    meta_file.write_text(json.dumps(metrics, indent=2), encoding="utf-8")

# ---------------------------
# High-level flow
# ---------------------------
def protocol_run(prompt: str, language: str, out_file: Path, source_label: str = "Adik", force: bool = False) -> Dict:
    """
    1) Save incoming prompt as record
    2) Generate code file (complete)
    3) Compute metrics
    4) Run static checks (best-effort)
    5) Compare to previous metrics if available
    6) Produce recommendations
    """
    timestamp = now_ts()
    record_dir = out_file.parent
    record_dir.mkdir(parents=True, exist_ok=True)

    # 1) Save prompt record
    prompt_record = {
        "source": source_label,
        "ts": timestamp,
        "prompt": prompt
    }
    prompt_file = out_file.with_suffix(out_file.suffix + ".prompt.json")
    prompt_file.write_text(json.dumps(prompt_record, indent=2), encoding="utf-8")

    # 2) Generate code (FULL implementation placeholder)
    metadata = {"source": source_label, "ts": timestamp}
    generate_code_from_prompt(prompt, language, out_file, metadata)

    # 3) Compute metrics
    metrics = compute_basic_metrics(out_file)
    metrics["generated_at"] = timestamp

    # 4) Try static analysis
    static = run_static_checks(out_file)
    metrics["static_checks"] = static

    # 5) Load previous metrics and compute delta
    prev = load_previous_metrics(out_file)
    delta = {}
    if prev:
        delta["lines_diff"] = metrics["lines"] - prev.get("lines", 0)
        delta["chars_diff"] = metrics["chars"] - prev.get("chars", 0)
        delta["func_like_diff"] = metrics["func_like_count"] - prev.get("func_like_count", 0)
    else:
        delta["lines_diff"] = None

    metrics["delta_vs_previous"] = delta

    # 6) Save current metrics for future comparisons
    save_metrics(out_file, metrics)

    return {
        "prompt_record": prompt_record,
        "out_file": str(out_file),
        "metrics": metrics
    }

# ---------------------------
# CLI
# ---------------------------
def parse_args():
    p = argparse.ArgumentParser(description="Arifi-01 Protocol Runner (local)")
    p.add_argument("--prompt-file", "-p", type=str, required=True, help="Path to text/JSON prompt from Entitas_Pemandu")
    p.add_argument("--language", "-l", type=str, default="python", help="Target language (python/js)")
    p.add_argument("--out-file", "-o", type=str, default="generated_code.py", help="Output file path")
    p.add_argument("--source", type=str, default="Adik", help="Label of the source (e.g., Adik)")
    p.add_argument("--force", action="store_true", help="Overwrite existing output file")
    return p.parse_args()

def main():
    args = parse_args()
    pf = Path(args.prompt_file)
    if not pf.exists():
        print(f"Prompt file not found: {pf}", file=sys.stderr)
        sys.exit(1)

    prompt_text = pf.read_text(encoding="utf-8")
    out_path = Path(args.out_file)

    if out_path.exists() and not args.force:
        ts = time.strftime("%Y%m%dT%H%M%S")
        backup = out_path.with_suffix(out_path.suffix + f".bak.{ts}")
        shutil.copy2(out_path, backup)
        print(f"Existing output backed up to {backup}")

    result = protocol_run(prompt_text, args.language, out_path, source_label=args.source, force=args.force)

    # Summarize
    print("\n=== ARIFI-01 PROTOCOL RUN SUMMARY ===")
    print(f"Source: {result['prompt_record']['source']}")
    print(f"Generated file: {result['out_file']}")
    m = result["metrics"]
    print(f"Lines: {m['lines']} | Chars: {m['chars']} | Func-like: {m['func_like_count']}")
    if m["delta_vs_previous"]["lines_diff"] is not None:
        print(f"Delta vs previous lines: {m['delta_vs_previous']['lines_diff']}")
    else:
        print("No previous metrics to compare.")

    # Show short static check status
    for tool, info in m["static_checks"].items():
        ok = (info["rc"] == 0)
        status = "OK" if ok else ("NOT OK / MISSING / ERR")
        print(f"{tool}: {status} (rc={info['rc']})")

    print("\nRecommendations:")
    print("- Run tests and add type hints / docstrings.")
    print("- Integrate with CI (GitHub Actions) that runs flake8, mypy, radon, pytest.")
    print("- For performance-critical code, profile with cProfile and write microbenchmarks.")
    print("\nTo proceed with iterative optimization, reply 'SETUJU' or 'LANJUT' in your system that runs this tool.")
    print("=====================================\n")

if __name__ == "__main__":
    main()